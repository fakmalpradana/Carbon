{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1KJnP7zSGQg5"
   },
   "source": [
    "## Binary semantic segmentation example using DCSwin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cFxHJWmXlcZk",
    "outputId": "b755d1a8-3650-42d9-8718-401b4458f049"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.2+cu118\n",
      "0.16.2+cu118\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(DEVICE)\n",
    "\n",
    "# determine if we will be pinning memory during data loading\n",
    "PIN_MEMORY = True if DEVICE == \"cuda\" else False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KVfaGZrWG63Q"
   },
   "source": [
    "#### CONFIGURE YOUR PATHS AND HYPERPARAMETERS FOR TRAINING BELOW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "OjlBC-raVM2K",
    "outputId": "b6950142-9c43-40bd-9edb-e10b3973f745"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\dev\\Carbon\n",
      "c:\\dev\\Carbon\\data/jambi\n",
      "c:\\dev\\Carbon\\data/jambi\\sample\\tile\n",
      "c:\\dev\\Carbon\\data/jambi\\sample\\mask\n",
      "c:\\dev\\Carbon\\data/jambi\\sample\\tile\n",
      "c:\\dev\\Carbon\\data/jambi\\sample\\mask\n",
      "c:\\dev\\Carbon\\data/jambi\\sample\\tile\n",
      "c:\\dev\\Carbon\\data/jambi\\sample\\mask\n",
      "c:\\dev\\Carbon\\trained_models\\dcswin-10-epochs-220424.pth\n",
      "c:\\dev\\Carbon\\plots\\dcswin-10-epochs-220424.png\n"
     ]
    }
   ],
   "source": [
    "GD_PATH = os.getcwd() # get current working directory for the repo\n",
    "print(GD_PATH)\n",
    "\n",
    "# PROVIDE PATH TO DOWNLOADED MAPAI DATASET\n",
    "DATASET_PATH = os.path.join(GD_PATH, \"data/jambi\")\n",
    "\n",
    "# DATASET_PATH = os.path.join(DATASET_PATH, \"mapai_full\") # create dataset path\n",
    "\n",
    "print(DATASET_PATH)\n",
    "\n",
    "TRAIN_IMG_DIR = os.path.join(DATASET_PATH, \"sample\", \"tile\")\n",
    "TRAIN_MASK_DIR = os.path.join(DATASET_PATH, \"sample\", \"mask\")\n",
    "\n",
    "print(TRAIN_IMG_DIR)\n",
    "print(TRAIN_MASK_DIR)\n",
    "\n",
    "VAL_IMG_DIR = os.path.join(DATASET_PATH, \"sample\", \"tile\")\n",
    "VAL_MASK_DIR = os.path.join(DATASET_PATH, \"sample\", \"mask\")\n",
    "\n",
    "print(VAL_IMG_DIR)\n",
    "print(VAL_MASK_DIR)\n",
    "\n",
    "TEST_IMG_DIR = os.path.join(DATASET_PATH, \"sample\", \"tile\")\n",
    "TEST_MASK_DIR = os.path.join(DATASET_PATH, \"sample\", \"mask\")\n",
    "\n",
    "print(TEST_IMG_DIR)\n",
    "print(TEST_MASK_DIR)\n",
    "\n",
    "# CONFIGURE MapAI DATASET\n",
    "NUM_CHANNELS = 3\n",
    "NUM_LEVELS  = 3\n",
    "NUM_CLASSES = 11\n",
    "\n",
    "# IMAGE SHAPE\n",
    "IMG_WIDTH = 512\n",
    "IMG_HEIGHT = 512\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------#\n",
    "\n",
    "# CONFIGURE parameters for training\n",
    "\n",
    "EPOCHS = 10\n",
    "init_lr = 1e-4 # learning rate\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "THRESHOLD  = 0.5\n",
    "base_output = \"out\"\n",
    "\n",
    "model_name = f\"dcswin-{EPOCHS}-epochs-220424.pth\" # provide name for model\n",
    "training_plot_name = f\"dcswin-{EPOCHS}-epochs-220424.png\"\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------#\n",
    "\n",
    "# OUTPUT PATHS\n",
    "\n",
    "# Trained model path\n",
    "MODEL_PATH = os.path.join(GD_PATH, \"trained_models\", model_name) # change depending on the number of epochs\n",
    "print(MODEL_PATH)\n",
    "PLOT_PATH  = os.path.join(GD_PATH, \"plots\", training_plot_name) # the folder to save future plots\n",
    "print(PLOT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IfSMUZbWWdJn",
    "tags": []
   },
   "source": [
    "### Load and read the MapAI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "    \n",
    "class AmbilDataset(Dataset):\n",
    "    def __init__(self, img_dir, mask_dir, transforms=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transforms = transforms\n",
    "        #self.images = os.listdir(img_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_dir)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        #img_path = os.path.join(self.img_dir, self.images[index])\n",
    "        #mask_path = os.path.join(self.mask_dir, self.images[index])\n",
    "\n",
    "        img_path = self.img_dir[index]\n",
    "        mask_path = self.mask_dir[index]\n",
    "        \n",
    "        image = np.array(Image.open(img_path).convert('RGB'))\n",
    "        mask = np.array(Image.open(mask_path).convert('L'), dtype=np.float32)\n",
    "        \n",
    "        b, g, r    = image[:, :, 0], image[:, :, 1], image[:, :, 2]\n",
    "        b = np.expand_dims(b, axis=-1)\n",
    "        g = np.expand_dims(g, axis=-1)\n",
    "        r = np.expand_dims(r, axis=-1)\n",
    "\n",
    "        mask[mask == 255.0] = 1.0\n",
    "\n",
    "        # merge channel\n",
    "        image = np.concatenate((b, g, r), axis=2)\n",
    "\n",
    "        # image = image5.transpose((2,0,1))\n",
    "        # mask = mask.transpose((2,0,1))\n",
    "        # print(f'ukuran input : {image.shape}')\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            # aug = self.transforms(image=image, mask=mask)\n",
    "            # image = aug['image']\n",
    "            # mask = aug['mask']\n",
    "            image = self.transforms(image)\n",
    "            mask = self.transforms(mask)\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AKXL9bO8WnNg"
   },
   "source": [
    "### Build DCSWIN architecture\n",
    "Downloaded from: https://github.com/WangLibo1995/GeoSeg/blob/main/geoseg/models/FTUNetFormer.py code changed for binary semantic segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "9urE3W1iWp7v"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "subfolder = os.path.join(GD_PATH, \"models\")\n",
    "sys.path.insert(0, subfolder)\n",
    "\n",
    "from utils import dcswin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22hbANvfWxmX",
    "tags": []
   },
   "source": [
    "### Training the segmentation model\n",
    "Below we append the paths for TRAIN/VAL/TEST sets - images/masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G2Jha-LCW0ir",
    "outputId": "fcad4c67-0851-42e6-ea68-6b6dd88d26c2"
   },
   "outputs": [],
   "source": [
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from imutils import paths\n",
    "import time\n",
    "\n",
    "# TRAINING\n",
    "train_images = sorted(list(paths.list_images(TRAIN_IMG_DIR)))\n",
    "train_masks = sorted(list(paths.list_images(TRAIN_MASK_DIR)))\n",
    "\n",
    "# VALIDATION\n",
    "val_images = sorted(list(paths.list_images(VAL_IMG_DIR)))\n",
    "val_masks = sorted(list(paths.list_images(VAL_MASK_DIR)))\n",
    "\n",
    "\n",
    "# TEST\n",
    "test_images = sorted(list(paths.list_images(TEST_IMG_DIR)))\n",
    "test_masks = sorted(list(paths.list_images(TEST_MASK_DIR)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gtqUNGR1XCa5",
    "tags": []
   },
   "source": [
    "### Define transformations\n",
    "\n",
    "I tried out different data augmentation techniques, including Horizontal Flip, Vertical Flip, Contrast, Brightness. They did not improve my results much, the validation and training loss were actually worse than without data augmentation techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ghW7Nj0OEQMc"
   },
   "source": [
    "https://pytorch.org/vision/stable/auto_examples/plot_transforms.html#sphx-glr-auto-examples-plot-transforms-py\n",
    "\n",
    "https://albumentations.ai/docs/getting_started/mask_augmentation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WR_dzdpCXCHY",
    "outputId": "4e9b1681-2846-489f-edf6-a6240af65563"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] found 7 examples in the TRAINING set...\n",
      "[INFO] found 7 examples in the VALIDATION set...\n",
      "[INFO] found 7 examples in the TEST set...\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as T\n",
    "\n",
    "# T.RandomHorizontalFlip(p=0.5),\n",
    "# T.RandomVerticalFlip(p=0.1),\n",
    "\n",
    "# Image augmentations applied\n",
    "transforms = T.Compose([T.ToPILImage(),\n",
    "                        T.Resize((IMG_HEIGHT,IMG_WIDTH)),\n",
    "                        T.ToTensor()])\n",
    "\n",
    "# create the train and test datasets\n",
    "trainDS = AmbilDataset(img_dir=train_images,\n",
    "                       mask_dir=train_masks,\n",
    "                       transforms=transforms)\n",
    "\n",
    "valDS = AmbilDataset(img_dir=val_images,\n",
    "                     mask_dir=val_masks,\n",
    "                     transforms=transforms)\n",
    "\n",
    "testDS = AmbilDataset(img_dir=test_images,\n",
    "                      mask_dir=test_masks,\n",
    "                      transforms=transforms)\n",
    "\n",
    "print(f\"[INFO] found {len(trainDS)} examples in the TRAINING set...\")\n",
    "print(f\"[INFO] found {len(valDS)} examples in the VALIDATION set...\")\n",
    "print(f\"[INFO] found {len(testDS)} examples in the TEST set...\")\n",
    "\n",
    "# create the training and test data loaders\n",
    "trainLoader = DataLoader(trainDS,\n",
    "                         shuffle=True,\n",
    "                         batch_size=BATCH_SIZE,\n",
    "                         pin_memory=PIN_MEMORY,\n",
    "                         num_workers=0)\n",
    "\n",
    "valLoader = DataLoader(valDS,\n",
    "                       shuffle=False,\n",
    "                       batch_size=BATCH_SIZE,\n",
    "                       pin_memory=PIN_MEMORY,\n",
    "                       num_workers=0)\n",
    "\n",
    "testLoader = DataLoader(testDS,\n",
    "                        shuffle=False,\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        pin_memory=PIN_MEMORY,\n",
    "                        num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tAO9M_R4XG6q",
    "tags": []
   },
   "source": [
    "### Initialize DCSWIN model for training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2IMsYzUaXJW7",
    "outputId": "5328f1e2-4a71-4c05-fa9e-23b768971315"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 7 7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train_loss': [], 'val_loss': [], 'test_loss': []}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = dcswin.DCSwin().to(DEVICE)\n",
    "\n",
    "# loss / optimizer\n",
    "lossFunction = BCEWithLogitsLoss()\n",
    "opt = Adam(model.parameters(), lr=init_lr)\n",
    "\n",
    "# calculate steps per epoch for train/val/test\n",
    "trainSteps = len(trainDS) // BATCH_SIZE \n",
    "valSteps = len(valDS) // BATCH_SIZE\n",
    "testSteps = len(testDS) // BATCH_SIZE\n",
    "\n",
    "print(trainSteps, valSteps, testSteps)\n",
    "\n",
    "# initialize a dictionary to store training history\n",
    "H = {\"train_loss\": [], \"val_loss\": [], \"test_loss\": []}\n",
    "H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "WEP-IVokbWQg"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xcjuKhMeXLU-",
    "tags": []
   },
   "source": [
    "### TRAINING THE MODEL\n",
    "\n",
    "Run this piece of code only if you want to train the model from scratch.\n",
    "\n",
    "Training locally: BATCH_SIZE  = 2 takes 5035 MB of GPU memory.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DCSwin(\n",
       "  (backbone): SwinTransformer(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0): BasicLayer(\n",
       "        (blocks): ModuleList(\n",
       "          (0): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): PatchMerging(\n",
       "          (reduction): Linear(in_features=512, out_features=256, bias=False)\n",
       "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicLayer(\n",
       "        (blocks): ModuleList(\n",
       "          (0-1): 2 x SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): PatchMerging(\n",
       "          (reduction): Linear(in_features=1024, out_features=512, bias=False)\n",
       "          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (2): BasicLayer(\n",
       "        (blocks): ModuleList(\n",
       "          (0-17): 18 x SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): PatchMerging(\n",
       "          (reduction): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "          (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (3): BasicLayer(\n",
       "        (blocks): ModuleList(\n",
       "          (0-1): 2 x SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (dcfam): DCFAM(\n",
       "      (conv4): Conv(\n",
       "        (0): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (conv1): Conv(\n",
       "        (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (lf4): Sequential(\n",
       "        (0): SeparableConvBNReLU(\n",
       "          (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(6, 6), dilation=(6, 6), groups=1024, bias=False)\n",
       "          (1): Conv2d(1024, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (2): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (3): ReLU()\n",
       "        )\n",
       "        (1): UpsamplingNearest2d(scale_factor=2.0, mode='nearest')\n",
       "        (2): SeparableConvBNReLU(\n",
       "          (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(12, 12), dilation=(12, 12), groups=768, bias=False)\n",
       "          (1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (3): ReLU()\n",
       "        )\n",
       "        (3): UpsamplingNearest2d(scale_factor=2.0, mode='nearest')\n",
       "      )\n",
       "      (lf3): Sequential(\n",
       "        (0): SeparableConvBNReLU(\n",
       "          (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(6, 6), dilation=(6, 6), groups=768, bias=False)\n",
       "          (1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (3): ReLU()\n",
       "        )\n",
       "        (1): UpsamplingNearest2d(scale_factor=2.0, mode='nearest')\n",
       "        (2): SeparableConvBNReLU(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(12, 12), dilation=(12, 12), groups=384, bias=False)\n",
       "          (1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (3): ReLU()\n",
       "        )\n",
       "        (3): UpsamplingNearest2d(scale_factor=2.0, mode='nearest')\n",
       "      )\n",
       "      (ca): SharedChannelAttention()\n",
       "      (pa): SharedSpatialAttention(\n",
       "        (query_conv): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (key_conv): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (value_conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (down12): DownConnection(\n",
       "        (convbn1): ConvBN(\n",
       "          (0): Conv2d(96, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (convbn2): ConvBN(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): ConvBN(\n",
       "          (0): Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (down231): DownConnection(\n",
       "        (convbn1): ConvBN(\n",
       "          (0): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (convbn2): ConvBN(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): ConvBN(\n",
       "          (0): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (down232): DownConnection(\n",
       "        (convbn1): ConvBN(\n",
       "          (0): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (convbn2): ConvBN(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): ConvBN(\n",
       "          (0): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (down34): DownConnection(\n",
       "        (convbn1): ConvBN(\n",
       "          (0): Conv2d(384, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (convbn2): ConvBN(\n",
       "          (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): ConvBN(\n",
       "          (0): Conv2d(384, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout2d(p=0.05, inplace=True)\n",
       "    (segmentation_head): Sequential(\n",
       "      (0): ConvBNReLU(\n",
       "        (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (1): Conv(\n",
       "        (0): Conv2d(96, 6, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (2): UpsamplingBilinear2d(scale_factor=4.0, mode='bilinear')\n",
       "    )\n",
       "    (up): Sequential(\n",
       "      (0): ConvBNReLU(\n",
       "        (0): Conv2d(192, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (1): UpsamplingNearest2d(scale_factor=2.0, mode='nearest')\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vWuUyLUgXPNf",
    "outputId": "34c6485b-838b-45cf-99da-601f7cfbc4d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training DCSwin ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [768, 768, 1, 1], expected input[1, 1024, 16, 16] to have 768 channels, but got 1024 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m (x, y) \u001b[38;5;241m=\u001b[39m (x\u001b[38;5;241m.\u001b[39mto(DEVICE), y\u001b[38;5;241m.\u001b[39mto(DEVICE))\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# perform a forward pass and calculate the training loss\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m loss \u001b[38;5;241m=\u001b[39m lossFunction(pred, y)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# calculate the accuracy\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\miniconda3\\envs\\mvt\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\miniconda3\\envs\\mvt\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\dev\\Carbon\\utils\\dcswin.py:933\u001b[0m, in \u001b[0;36mDCSwin.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    931\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m    932\u001b[0m     x1, x2, x3, x4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackbone(x)\n\u001b[1;32m--> 933\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx4\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    934\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\miniconda3\\envs\\mvt\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\miniconda3\\envs\\mvt\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\dev\\Carbon\\utils\\dcswin.py:882\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[1;34m(self, x1, x2, x3, x4)\u001b[0m\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x1, x2, x3, x4):\n\u001b[1;32m--> 882\u001b[0m     out1, out2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdcfam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx4\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    883\u001b[0m     x \u001b[38;5;241m=\u001b[39m out1 \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup(out2)\n\u001b[0;32m    884\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\miniconda3\\envs\\mvt\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\miniconda3\\envs\\mvt\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\dev\\Carbon\\utils\\dcswin.py:851\u001b[0m, in \u001b[0;36mDCFAM.forward\u001b[1;34m(self, x1, x2, x3, x4)\u001b[0m\n\u001b[0;32m    850\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x1, x2, x3, x4):\n\u001b[1;32m--> 851\u001b[0m     out4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv4\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx4\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown34(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpa(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown232(x2)))\n\u001b[0;32m    852\u001b[0m     out3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpa(x3) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown231(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mca(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown12(x1)))\n\u001b[0;32m    853\u001b[0m     out2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mca(x2) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlf4(out4)\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\miniconda3\\envs\\mvt\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\miniconda3\\envs\\mvt\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\miniconda3\\envs\\mvt\\lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\miniconda3\\envs\\mvt\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\miniconda3\\envs\\mvt\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\miniconda3\\envs\\mvt\\lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\miniconda3\\envs\\mvt\\lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [768, 768, 1, 1], expected input[1, 1024, 16, 16] to have 768 channels, but got 1024 channels instead"
     ]
    }
   ],
   "source": [
    "# loop over epochs\n",
    "print(\"[INFO] training DCSwin ...\")\n",
    "startTime = time.time()\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "    model.train()\n",
    "\n",
    "    # initialize total training and validation loss\n",
    "    totalTrainLoss = 0\n",
    "    totalValLoss = 0\n",
    "    totalTrainAcc = 0\n",
    "    totalValAcc = 0\n",
    "\n",
    "    # loop over the training set\n",
    "    for (i, (x, y)) in enumerate(trainLoader):\n",
    "        # send output to device\n",
    "        (x, y) = (x.to(DEVICE), y.to(DEVICE))\n",
    "\n",
    "        # perform a forward pass and calculate the training loss\n",
    "        pred = model(x)\n",
    "        loss = lossFunction(pred, y)\n",
    "        \n",
    "        # calculate the accuracy\n",
    "        acc = ((pred > 0.5) == y).float().mean()\n",
    "\n",
    "        # kill previously accumulated gradients then\n",
    "        # perform backpropagation and update model parameters\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        # add the loss to the total training loss\n",
    "        totalTrainLoss += loss\n",
    "        totalTrainAcc += acc\n",
    "\n",
    "    # switch of autograd\n",
    "    with torch.no_grad():\n",
    "        # set the model in evaluation mode\n",
    "        model.eval()\n",
    "\n",
    "        # loop over the validation set\n",
    "        for (x, y) in valLoader:\n",
    "             # send the input to the device\n",
    "            (x, y) = (x.to(DEVICE), y.to(DEVICE))\n",
    "\n",
    "            # make the predictions and calculate the validation loss\n",
    "            pred = model(x)\n",
    "            totalValLoss += lossFunction(pred, y)\n",
    "            \n",
    "            acc = ((pred > 0.5) == y).float().mean()\n",
    "            totalValAcc += acc\n",
    "\n",
    "    # calculate the average training and validation loss\n",
    "    avgTrainLoss = totalTrainLoss / trainSteps\n",
    "    avgValLoss = totalValLoss / valSteps\n",
    "    avgTrainAcc = totalTrainAcc / trainSteps\n",
    "    avgValAcc = totalValAcc / valSteps\n",
    "        \n",
    "    # update our training history\n",
    "    H[\"train_loss\"].append(avgTrainLoss.cpu().detach().numpy())\n",
    "    H[\"val_loss\"].append(avgValLoss.cpu().detach().numpy())\n",
    "\n",
    "    # print the model training and validation information\n",
    "    print(\"[INFO] EPOCH: {}/{}\".format(epoch + 1, EPOCHS))\n",
    "    print(\"Train loss: {:.6f}, Val loss: {:.4f}\".format(avgTrainLoss, avgValLoss))\n",
    "    # print(\"Train loss: {:.6f}, Train acc: {:.6f}, Val loss: {:.4f}, Val acc: {:.4f}\".format(avgTrainLoss, avgTrainAcc, avgValLoss, avgValAcc))\n",
    "        \n",
    "# display the total time needed to perform the training\n",
    "endTime = time.time()\n",
    "print(\"[INFO] total time taken to train the model: {:.2f}s\".format(endTime - startTime))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train loss: 0.001194, Val loss: 0.0013\n",
    "[INFO] total time taken to train the model: 27115.38s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CsJoOVn11rs9",
    "outputId": "c9a36460-f773-4771-cfd7-fec78711d8cc"
   },
   "outputs": [],
   "source": [
    "H # show traning/val loss history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U6ChLXHuXZHA",
    "tags": []
   },
   "source": [
    "### Plot the training and validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "id": "j04HfubrXYvX",
    "outputId": "5416f4a7-2647-40e9-ce25-cac7909dea50"
   },
   "outputs": [],
   "source": [
    "# plot the training loss\n",
    "print(MODEL_PATH)\n",
    "print(PLOT_PATH)\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(H[\"train_loss\"], label=\"train_loss\")\n",
    "plt.plot(H[\"val_loss\"], label=\"val_loss\")\n",
    "plt.title(\"Training Loss on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig(PLOT_PATH)\n",
    "# serialize the model to disk\n",
    "torch.save(model, MODEL_PATH) # saves the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Y6Fx2oaWr0q",
    "tags": []
   },
   "source": [
    "### Prediction part\n",
    "\n",
    "Here the trained model is loaded and use for prediction on test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qYh4flMu7O-m",
    "outputId": "94909220-5b1f-43ad-b52f-d37bbaa270fd"
   },
   "outputs": [],
   "source": [
    "# Load saved model for prediction\n",
    "\n",
    "print(MODEL_PATH)\n",
    "\n",
    "model = torch.load(MODEL_PATH) # add MODEL_PATH after training\n",
    "print(\"model loaded for prediction\")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Provide test images for MapAI Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTIONS_DIR = os.path.join(GD_PATH, \"predictions_fullimage_061023\")\n",
    "PREDICTIONS_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Make predictions on the entire MapAI dataset\n",
    "\n",
    "Make predictions on test images and save them to the folder named predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gc\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# PLOTTING PREDICTIONS AS SINGLE IMAGES\n",
    "\n",
    "# Output folder for the predictions\n",
    "output_folder = PREDICTIONS_DIR + \"/\" # check for Windows to save predictions inside the folder\n",
    "\n",
    "# PLOT TEST IMAGES as RGB\n",
    "for n in range(len(test_images)):\n",
    "  gc.collect()\n",
    "  # Test image number\n",
    "  testImgName = str(Path(test_images[n]).stem) + '.tif'\n",
    "  #print('#', testImgName)\n",
    "\n",
    "   # Make predicton on a test image specified with counter n\n",
    "  test_img = test_images[n]\n",
    "  test_img_input = np.expand_dims(test_img, 0)\n",
    "  #print('#', test_img_input[0])\n",
    "\n",
    "  # PyTorch --> works\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    image = cv2.imread(test_img_input[0])\n",
    "    image = cv2.resize(image, dsize = (IMG_WIDTH, IMG_HEIGHT), interpolation=cv2.INTER_CUBIC)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = image.astype(\"float32\") / 255\n",
    "    \n",
    "    # print('SIZE: ', image.shape)\n",
    "\n",
    "    # make the channel axis to be the leading one, add batch dimension\n",
    "    image = np.transpose(image, (2, 0, 1))\n",
    "    # create a PyTorch tensor\n",
    "    image = np.expand_dims(image, 0)\n",
    "    # flash the tensor to the device\n",
    "    image  = torch.from_numpy(image).to(DEVICE)\n",
    "\n",
    "    # make the prediction\n",
    "    predMask = model(image).squeeze()\n",
    "    # pass result through sigmoid\n",
    "    predMask = torch.sigmoid(predMask)\n",
    "\n",
    "    # convert result to numpy array\n",
    "    predMask = predMask.cpu().numpy()\n",
    "\n",
    "    # filter out the weak predictions and convert them to integers\n",
    "    predMask = (predMask > THRESHOLD) * 255\n",
    "    predMask = predMask.astype(np.uint8)\n",
    "\n",
    "    # generate image from array\n",
    "    pIMG = Image.fromarray(predMask)\n",
    "    pIMG.save(str(output_folder + testImgName))\n",
    "\n",
    "    #print('Prediction:', testImgName, 'saved to:', output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Make predictions on single images by choice\n",
    "\n",
    "Change the parameter n to choose which image to plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = PREDICTIONS_DIR + \"/\" + \"*.tif\"\n",
    "\n",
    "predictions = glob.glob(output_folder)\n",
    "predictions.sort()\n",
    "print(\"# IMAGES for prediction: \", len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bq7BlbdrcgPB",
    "outputId": "6860afd9-da51-4911-d975-5a3ed78e01e1"
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "import cv2\n",
    "\n",
    "# print(\"Choosen n can be from 0 o 1367! \")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "n = 91 # change this number depending on which image you want to test\n",
    "\n",
    "fig = plt.figure(figsize=(18,12))\n",
    "\n",
    "ax1 = fig.add_subplot(131)\n",
    "ax1.set_title('RGB image: ')\n",
    "image = cv2.imread(test_images[n])[:,:,::-1]\n",
    "ax1.imshow(image)\n",
    "ax1.set_axis_off()\n",
    "\n",
    "ax2 = fig.add_subplot(132)\n",
    "ax2.set_title('Ground truth: ')\n",
    "image = cv2.imread(test_masks[n])[:,:,::-1]\n",
    "#image *= 255\n",
    "ax2.imshow(image)\n",
    "ax2.set_axis_off()\n",
    "\n",
    "ax3 = fig.add_subplot(133)\n",
    "ax3.set_title('Prediction: ')\n",
    "image = cv2.imread(predictions[n])[:,:,::-1]\n",
    "ax3.imshow(image)\n",
    "ax3.set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Predict Full Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tentukan lokasi direktori\n",
    "FULL_IMG_DIR = os.path.join(GD_PATH, \"data_full_img\", \"img1\")\n",
    "FULL_MASK_DIR = os.path.join(GD_PATH, \"data_full_img\", \"mask\")\n",
    "FULL_IMG_TILES_DIR = os.path.join(GD_PATH, \"data_full_img\", \"img1_tiles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTIONS_FULL_DIR = os.path.join(GD_PATH, \"predictions_fullimage_061023\")\n",
    "PREDICTIONS_FULL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_img_dir = os.path.join(FULL_IMG_DIR, \"Kav_6_1.tif\") #ganti sesuai nama file orthophoto \n",
    "\n",
    "patch_size = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Tilling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import patchify as p\n",
    "\n",
    "def patchData(img, patch_dim:int, step_size:float):\n",
    "    # pembulatan channel img\n",
    "    ch1 = np.ceil(img.shape[0]/patch_dim).astype(int)\n",
    "    ch2 = np.ceil(img.shape[1]/patch_dim).astype(int)\n",
    "\n",
    "    # menambahkan kolom dan baris kosong pd gambar\n",
    "    arr0 = np.zeros((ch1*patch_dim, ch2*patch_dim, 3))\n",
    "    arr0[:img.shape[0], :img.shape[1]] += img\n",
    "    arr = arr0.astype(np.uint8)\n",
    "\n",
    "    # patch image\n",
    "    patch_shape = (patch_dim, patch_dim, 3)\n",
    "    patches = p.patchify(arr, patch_shape, step=int(patch_dim*step_size))\n",
    "\n",
    "    img_patches = []\n",
    "    for i in range(patches.shape[0]):\n",
    "        for j in range(patches.shape[1]):\n",
    "            img_patches.append(patches[i,j,0,:,:,:])\n",
    "\n",
    "    return np.array(img_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(full_img_dir)\n",
    "\n",
    "print(f'size ori : {img.shape}')\n",
    "\n",
    "patch = patchData(img, patch_size, 1)\n",
    "print(f'patch size : {patch.shape}')\n",
    "print(patch.shape[0])\n",
    "\n",
    "for i in range(patch.shape[0]):\n",
    "    cv2.imwrite(FULL_IMG_TILES_DIR + f'/img_{i+1000}.png', patch[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Predicting full images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Full Image\n",
    "\n",
    "full_images = sorted(list(paths.list_images(FULL_IMG_TILES_DIR)))\n",
    "#full_masks = sorted(list(paths.list_images(FULL_MASK_DIR)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gc\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import imagesize\n",
    "import cv2\n",
    "\n",
    "# PLOTTING PREDICTIONS AS SINGLE IMAGES\n",
    "\n",
    "# Output folder for the predictions\n",
    "output_folder_full = PREDICTIONS_FULL_DIR + \"/\" # check for Windows to save predictions inside the folder\n",
    "\n",
    "# PLOT TEST IMAGES as RGB\n",
    "for n in range(len(full_images)):\n",
    "  gc.collect()\n",
    "  # Test image number\n",
    "  testImgName = str(Path(full_images[n]).stem) + '.tif'\n",
    "  #IMG_WIDTH, IMG_HEIGHT = imagesize.get(testImgName)\n",
    "    #print('#', testImgName)\n",
    "\n",
    "   # Make predicton on a test image specified with counter n\n",
    "  test_img = full_images[n]\n",
    "  test_img_input = np.expand_dims(test_img, 0)\n",
    "  #print('#', test_img_input[0])\n",
    "\n",
    "  # PyTorch --> works\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    image = cv2.imread(test_img_input[0])\n",
    "    image = cv2.resize(image, dsize = (patch_size, patch_size), interpolation=cv2.INTER_CUBIC)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = image.astype(\"float32\") / 255\n",
    "    \n",
    "    # print('SIZE: ', image.shape)\n",
    "\n",
    "    # make the channel axis to be the leading one, add batch dimension\n",
    "    image = np.transpose(image, (2, 0, 1))\n",
    "    # create a PyTorch tensor\n",
    "    image = np.expand_dims(image, 0)\n",
    "    # flash the tensor to the device\n",
    "    image  = torch.from_numpy(image).to(DEVICE)\n",
    "\n",
    "    # make the prediction\n",
    "    predMask = model(image).squeeze()\n",
    "    # pass result through sigmoid\n",
    "    predMask = torch.sigmoid(predMask)\n",
    "\n",
    "    # convert result to numpy array\n",
    "    predMask = predMask.cpu().numpy()\n",
    "\n",
    "    # filter out the weak predictions and convert them to integers\n",
    "    predMask = (predMask > THRESHOLD) * 255\n",
    "    predMask = predMask.astype(np.uint8)\n",
    "\n",
    "    # generate image from array\n",
    "    pIMG = Image.fromarray(predMask)\n",
    "    pIMG.save(str(output_folder_full + testImgName))\n",
    "\n",
    "    print('Prediction:', testImgName, 'saved to:', output_folder_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Predict using Smooth Tiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from patchify import patchify, unpatchify\n",
    "from PIL import Image\n",
    "#import segmentation_models as sm\n",
    "\n",
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "#scaler = MinMaxScaler()\n",
    "\n",
    "from smooth_tiled_predictions import predict_img_with_smooth_windowing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_img_dir = os.path.join(FULL_IMG_DIR, \"bogor_images.tif\")\n",
    "full_mask_dir = os.path.join(FULL_MASK_DIR, \"bogor_masks.tif\")\n",
    "\n",
    "img = cv2.imread(full_img_dir, 1)\n",
    "#original_mask = cv2.imread(full_mask_dir, 1)\n",
    "#original_mask = cv2.cvtColor(original_mask,cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "subfolder = os.path.join(GD_PATH, \"models\")\n",
    "sys.path.insert(0, subfolder)\n",
    "\n",
    "import DCSwin_model\n",
    "\n",
    "print(MODEL_PATH)\n",
    "\n",
    "model = torch.load(MODEL_PATH) # add MODEL_PATH after training\n",
    "print(\"model loaded for prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of patches\n",
    "patch_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img.shape[1])\n",
    "print(img.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE_X = (img.shape[1]//patch_size)*patch_size #Nearest size divisible by our patch size\n",
    "SIZE_Y = (img.shape[0]//patch_size)*patch_size #Nearest size divisible by our patch size\n",
    "print(SIZE_X)\n",
    "print(SIZE_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE_X = (img.shape[1]//patch_size)*patch_size #Nearest size divisible by our patch size\n",
    "SIZE_Y = (img.shape[0]//patch_size)*patch_size #Nearest size divisible by our patch size\n",
    "large_img = Image.fromarray(img)\n",
    "large_img = large_img.crop((0 ,0, SIZE_X, SIZE_Y))  #Crop from top left corner\n",
    "#image = image.resize((SIZE_X, SIZE_Y))  #Try not to resize for semantic segmentation\n",
    "large_img = np.array(large_img)     \n",
    "\n",
    "\n",
    "patches_img = patchify(large_img, (patch_size, patch_size, 3), step=patch_size)  #Step=256 for 256 patches means no overlap\n",
    "patches_img = patches_img[:,:,0,:,:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### BUILDING FOOTPRINT REGULARIZATION\n",
    "\n",
    "Used repo: https://github.com/zorzi-s/projectRegularization\n",
    "\n",
    "git clone the repo to the folder where your notebook is stored. To get curent working directory use os.getcwd().\n",
    "\n",
    "The pretrained weights need to be downloaded from the provided link and saved into the folder pretrained_weighs that is inside projectRegularization:\n",
    "\n",
    "https://drive.google.com/drive/folders/1IPrDpvFq9ODW7UtPAJR_T-gGzxDat_uu\n",
    "\n",
    "Next step is to generate a Python file to locate the necessary pretrained weights from projectRegularization. The code below was only tested on Ubuntu, not on Windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE NECESSARY PATHS FOR REGULARIZATION PART\n",
    "\n",
    "projectRegDir = os.path.join(GD_PATH, \"projectRegularization\")\n",
    "print(projectRegDir)\n",
    "\n",
    "ptw = os.path.join(projectRegDir, \"pretrained_weights\") \n",
    "print(ptw)\n",
    "\n",
    "# OUTPUT REGULARIZATIONS DIR\n",
    "REGULARIZATION_DIR = os.path.join(GD_PATH, \"regularizations_061023\") + \"/\"\n",
    "print(REGULARIZATION_DIR)\n",
    "\n",
    "# GET THE PATHS FOR TRAINED GAN MODELS\n",
    "ENCODER = os.path.join(ptw, \"E140000_e1\")\n",
    "GENERATOR = os.path.join(ptw, \"E140000_net\")\n",
    "\n",
    "print(ENCODER)\n",
    "print(GENERATOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A NEW variables.py WITH USERS PATHS\n",
    "\n",
    "with open(projectRegDir + 'variables.py', 'w') as f:\n",
    "    f.write('# CONFIGURE THE PATHS HERE: \\n\\n')\n",
    "    f.write('# TRAINING \\n')\n",
    "    f.write('DATASET_RGB = ' + '\"' + str(TRAIN_IMG_DIR + '*.tif' + '\"') + '\\n')\n",
    "    f.write('DATASET_GTI = ' + '\"' + str(TRAIN_MASK_DIR + '*.tif' + '\"') + '\\n')\n",
    "    f.write('DATASET_SEG = ' + '\"' + str(PREDICTIONS_DIR + '*.tif' + '\"') + '\\n')\n",
    "    f.write('\\n')\n",
    "    f.write('DEBUG_DIR = ' + '\"' + str('./debug/') + '\"' + '\\n')\n",
    "    f.write('\\n')\n",
    "    f.write('# INFERENCE \\n')\n",
    "    f.write('INF_RGB = ' + '\"' + str(TEST_IMG_DIR + '*.tif' + '\"') + '\\n')\n",
    "    f.write('INF_SEG = ' + '\"' + str(PREDICTIONS_DIR + '*.tif' + '\"') + '\\n')\n",
    "    f.write('INF_OUT = ' + '\"' + str(REGULARIZATION_DIR + '\"') + '\\n')\n",
    "    f.write('\\n')\n",
    "    f.write('MODEL_ENCODER = ' + '\"' + str(ENCODER) + '\"' + '\\n')\n",
    "    f.write('MODEL_GENERATOR = ' + '\"' + str(GENERATOR) + '\"' + '\\n')\n",
    "    f.close()\n",
    " \n",
    "print(\"variables.py created with users paths...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run projectRegularization\n",
    "\n",
    "Takes around 6-8 minutes.\n",
    "\n",
    "You only need to change the command below and replace it with the absolute path for regularize.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python /data/private/BPN_AI/mapAI-regularization/projectRegularization/regularize.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Compare predictions and regularizations on a single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Regularizations to plot and compare results\n",
    "\n",
    "regularizations = glob.glob(REGULARIZATION_DIR + \"*.tif\")\n",
    "regularizations.sort()\n",
    "\n",
    "print(\"# of predicted images: \", len(predictions))\n",
    "print(\"# of regularized images: \", len(regularizations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to plot RGB, GT, PREDICTION and REGULARIZATION in a single plot for comparison.\n",
    "\n",
    "Change parameter n accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICTIONS_DIR = os.path.join(GD_PATH, \"predictions_25ep\")\n",
    "# output_folder = PREDICTIONS_DIR + \"/\" + \"*.tif\"\n",
    "\n",
    "# predictions = glob.glob(output_folder)\n",
    "# predictions.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "\n",
    "fig = plt.figure(figsize=(18,12))\n",
    "ax1 = fig.add_subplot(141)\n",
    "\n",
    "ax1.set_title('RGB: ')\n",
    "image = cv2.imread(test_images[n])[:,:,::-1]\n",
    "ax1.imshow(image)\n",
    "ax1.set_axis_off()\n",
    "\n",
    "# ax2 = fig.add_subplot(142)\n",
    "# ax2.set_title('Ground truth: ')\n",
    "# image = cv2.imread(test_masks[n])[:,:,::-1]\n",
    "# #image *= 255\n",
    "# ax2.imshow(image)\n",
    "# ax2.set_axis_off()\n",
    "\n",
    "ax3 = fig.add_subplot(143)\n",
    "ax3.set_title('Prediction: ')\n",
    "image = cv2.imread(predictions[n])[:,:,::-1]\n",
    "ax3.imshow(image)\n",
    "ax3.set_axis_off()\n",
    "\n",
    "ax4 = fig.add_subplot(144)\n",
    "ax4.set_title('Regularization: ')\n",
    "image = cv2.imread(regularizations[n])[:,:,::-1]\n",
    "ax4.imshow(image)\n",
    "ax4.set_axis_off()\n",
    "\n",
    "# DEFINE PATH FOR PLOTS TO BE SAVED\n",
    "figPath = GD_PATH + \"/\" + \"plots\" + \"/\" \"compare-\" + str(n) + \".png\"\n",
    "print(figPath)\n",
    "\n",
    "# Save plot\n",
    "fig.savefig(figPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Combine patches into one full prediction image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_images = sorted(list(paths.list_images(FULL_IMG_TILES_DIR)))\n",
    "num_patches = len(full_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTIONS_FULL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTIONS_REG_FULL_DIR = os.path.join(GD_PATH, \"regularizations_051023\")\n",
    "PREDICTIONS_REG_FULL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tentukan nama file hasil prediksi\n",
    "full_pred_dir = \"combined_kav61_image_061023.tif\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "original_image = cv2.imread(full_img_dir)\n",
    "\n",
    "height_ori, width_ori, channels = original_image.shape\n",
    "\n",
    "result_height = height_ori + (patch_size - (height_ori-(patch_size*int(height_ori/patch_size))))\n",
    "result_width = width_ori + (patch_size - (width_ori-(patch_size*int(width_ori/patch_size))))\n",
    "\n",
    "print(width_ori)\n",
    "print(height_ori)\n",
    "print(result_width)\n",
    "print(result_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# # Define the folder containing the images in numerical order\n",
    "# image_folder = \"images/\"\n",
    "\n",
    "# # Define the dimensions of the resulting single image\n",
    "# result_width = 20992  # Set your desired width\n",
    "# result_height = 11776  # Set your desired height\n",
    "\n",
    "# Create a blank result image with the specified dimensions\n",
    "result_image = Image.new(\"I\", (result_width, result_height))\n",
    "\n",
    "# List the image files in the folder and sort them numerically\n",
    "# image_files = [f\"{image_folder}{i}.png\" for i in range(1, len(os.listdir(image_folder)) + 1)]\n",
    "# image_files.sort()\n",
    "image_files = sorted(list(paths.list_images(PREDICTIONS_REG_FULL_DIR)))\n",
    "\n",
    "# Initialize coordinates for pasting images in the result image\n",
    "x, y = 0, 0\n",
    "\n",
    "# Iterate through the image files and paste them into the result image\n",
    "for image_file in image_files:\n",
    "    image = Image.open(image_file)\n",
    "    \n",
    "    # Resize the image to fit within the specified dimensions\n",
    "    image.thumbnail((result_width, result_height))\n",
    "    \n",
    "    # Paste the resized image into the result image at the current position\n",
    "    result_image.paste(image, (x, y))\n",
    "    \n",
    "    # Update the x-coordinate for the next image\n",
    "    x += image.width\n",
    "    \n",
    "    # If the x-coordinate exceeds the result_width, reset it and move to the next row\n",
    "    if x >= result_width:\n",
    "        x = 0\n",
    "        y += image.height\n",
    "\n",
    "#result_image = result_image.crop(0,0,width_ori,height_ori)\n",
    "\n",
    "# Save the resulting single image\n",
    "result_image.save(full_pred_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Set Geotransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Open the PNG file\n",
    "png_image = Image.open(full_pred_dir)\n",
    "full_pred_dir = full_pred_dir.replace('.png', '.tif')\n",
    "\n",
    "# Convert the PNG image to a TIFF image\n",
    "# You can specify the output file format as 'TIFF' or 'TIF'\n",
    "png_image.save(full_pred_dir, format='TIFF')\n",
    "\n",
    "# Close the PNG image\n",
    "png_image.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from osgeo import gdal\n",
    "\n",
    "# Open the source raster dataset\n",
    "src_dataset = gdal.Open(full_img_dir, gdal.GA_ReadOnly)\n",
    "\n",
    "# Get the geotransform from the source dataset\n",
    "geotransform = src_dataset.GetGeoTransform()\n",
    "\n",
    "# Close the source dataset\n",
    "src_dataset = None\n",
    "\n",
    "# Open a new or existing target raster dataset\n",
    "# Replace 'target.tif' with the path to your target dataset\n",
    "dst_dataset = gdal.Open(full_pred_dir, gdal.GA_Update)\n",
    "\n",
    "# Set the geotransform for the target dataset\n",
    "dst_dataset.SetGeoTransform(geotransform)\n",
    "\n",
    "# Close the target dataset to save changes\n",
    "dst_dataset = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### VECTORIZING THE REGULARIZED BUILDING MASKS with GDAL\n",
    "\n",
    "GDAL: https://gdal.org/'\n",
    "\n",
    "GDAL: https://www.youtube.com/watch?v=q3DLdMj5zLA\n",
    "\n",
    "I do not know if it is possible to install GDAL on WINDOWS inside a conda environment.\n",
    "\n",
    "On Ubuntu you have to follow these steps:\n",
    "\n",
    "\n",
    "\n",
    "Specific process for installation: https://stackoverflow.com/questions/44005694/no-module-named-gdal\n",
    "\n",
    "- sudo apt-get update && sudo apt upgrade -y && sudo apt autoremove \n",
    "- sudo apt-get install -y cdo nco gdal-bin libgdal-dev-\n",
    "- python -m pip install --upgrade pip setuptools wheel\n",
    "- python -m pip install --upgrade gdal\n",
    "- conda install -c conda forge libgdal\n",
    "- conda install -c conda-forge libgdal\n",
    "- conda install -c conda-forge gdal\n",
    "- conda install tiledb=2.2\n",
    "- conda install poppler\n",
    "\n",
    "When you have this you can hopefully vectorize the detected masks quite easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fname_from_path(path):\n",
    "    \"\"\"\n",
    "    Given a path, returns the filename after the last frontslash character.\n",
    "    \"\"\"\n",
    "    return path.rsplit('/', 1)[-1]\n",
    "\n",
    "def get_fname_no_extension(path):\n",
    "    \"\"\"\n",
    "    Given a path, returns the filename without its extension.\n",
    "    \"\"\"\n",
    "    filename, extension = os.path.splitext(path)\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import osgeo\n",
    "from osgeo import gdal\n",
    "from osgeo import ogr\n",
    "print('GDAL version: ', osgeo.gdal.__version__)\n",
    "\n",
    "# Choose which image to vectorize\n",
    "n  = 1\n",
    "\n",
    "#input = Image.open('combined_image_051023a.tif')\n",
    "input = regularizations[n]\n",
    "print()\n",
    "print(\"INPUT: \", input)\n",
    "\n",
    "# print(get_fname_no_extension(input))\n",
    "\n",
    "# out\n",
    "output = get_fname_from_path(get_fname_no_extension(input)) + \".gpkg\"\n",
    "print(\"OUTPUT: \", output)\n",
    "\n",
    "# Open image with GDAl driver\n",
    "ds = gdal.Open(input)\n",
    "# Get the band\n",
    "band = ds.GetRasterBand(1)\n",
    "\n",
    "# Create the output shapefile\n",
    "driver = ogr.GetDriverByName(\"GPKG\")\n",
    "out_ds = driver.CreateDataSource(output)\n",
    "out_layer = out_ds.CreateLayer(output, geom_type=ogr.wkbPolygon)\n",
    "\n",
    "# Add a field to the layer to store the pixel values\n",
    "field_defn = ogr.FieldDefn(\"Pix_Value\", ogr.OFTInteger)\n",
    "out_layer.CreateField(field_defn)\n",
    "\n",
    "# Polygonize the PNG file\n",
    "gdal.Polygonize(band, None, out_layer, 0, [], callback=None)\n",
    "\n",
    "# Close the input and output files\n",
    "out_ds = None\n",
    "ds = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the builing detection case we need to only keep the vectors with pixel value 255. Easiest solution is to use: Extract by attribute. The Python solution with GDAL can be found below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ogr2ogr -where ID=\"1\" outfile.gpkg infile.\n",
    "\n",
    "# RUN from the command line inside Ubuntu\n",
    "!ogr2ogr -where Pix_Value=\"255\" bergen_-5943_1104B.gpkg bergen_-5943_1104.gpkg"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "5614dd747bc595cf94d4c937a609d8df6c75b545807dd5ca7f02df8b67f4ea7c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
